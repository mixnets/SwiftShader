{
  "comments": [
    {
      "key": {
        "uuid": "e9636427_b54eca78",
        "filename": "src/Vulkan/VkDescriptorSetLayout.hpp",
        "patchSetId": 6
      },
      "lineNbr": 37,
      "author": {
        "id": 6982
      },
      "writtenOn": "2019-06-12T19:32:24Z",
      "side": 1,
      "message": "This in every descriptor isn\u0027t ideal... not going to block over it",
      "revId": "998f397d80b59cc8521adf88a73e0cb3d851ccf8",
      "serverId": "aea13c4a-0b89-3eca-aee9-e193b1b77aa4",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "54ce6f70_8618cb5d",
        "filename": "src/Vulkan/VkDescriptorSetLayout.hpp",
        "patchSetId": 6
      },
      "lineNbr": 37,
      "author": {
        "id": 5050
      },
      "writtenOn": "2019-06-12T19:49:15Z",
      "side": 1,
      "message": "I\u0027m open to ideas of where to put this.",
      "parentUuid": "e9636427_b54eca78",
      "revId": "998f397d80b59cc8521adf88a73e0cb3d851ccf8",
      "serverId": "aea13c4a-0b89-3eca-aee9-e193b1b77aa4",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "3ef52dac_4e3af8d3",
        "filename": "src/Vulkan/VkDescriptorSetLayout.hpp",
        "patchSetId": 6
      },
      "lineNbr": 37,
      "author": {
        "id": 5005
      },
      "writtenOn": "2019-07-16T15:28:38Z",
      "side": 1,
      "message": "Many months ago, before our realization that sampler and image view descriptors can be mixed and matched, we considered having the descriptor essentially *be* the routine function pointer (with most of the state backed in already).\n\nSo instead of storing a single function pointer, providing access to the routine cache through the descriptor doesn\u0027t seem unreasonable. We could even have a distinct cache per descriptor, although we\u0027d lose the ability to keep the total number of routines reasonable, and we wouldn\u0027t be able to reuse routines for state that\u0027s identical.\n\nEither way this LGTM for now.",
      "parentUuid": "54ce6f70_8618cb5d",
      "revId": "998f397d80b59cc8521adf88a73e0cb3d851ccf8",
      "serverId": "aea13c4a-0b89-3eca-aee9-e193b1b77aa4",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "25ac7caf_8aa219d6",
        "filename": "src/Vulkan/VkDescriptorSetLayout.hpp",
        "patchSetId": 6
      },
      "lineNbr": 37,
      "author": {
        "id": 5050
      },
      "writtenOn": "2019-07-16T15:52:12Z",
      "side": 1,
      "message": "Ack",
      "parentUuid": "3ef52dac_4e3af8d3",
      "revId": "998f397d80b59cc8521adf88a73e0cb3d851ccf8",
      "serverId": "aea13c4a-0b89-3eca-aee9-e193b1b77aa4",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "585e47c0_4a66a6e6",
        "filename": "src/Vulkan/VkDevice.hpp",
        "patchSetId": 6
      },
      "lineNbr": 79,
      "author": {
        "id": 7183
      },
      "writtenOn": "2019-06-12T17:51:10Z",
      "side": 1,
      "message": "To be clear, I\u0027m suggesting you fold these two into a single function:\n\n  rr::Routine* getOrCreateSamplingRoutine(Key key, std::function\u003crr::Routine*()\u003e create);\n\nYou then don\u0027t need to expose the mutex, there\u0027s potential in the future for reducing the mutex lock down to the single requested routine, and there\u0027s no chance of accidentally forgetting to lock the mutex.",
      "range": {
        "startLine": 78,
        "startChar": 0,
        "endLine": 79,
        "endChar": 44
      },
      "revId": "998f397d80b59cc8521adf88a73e0cb3d851ccf8",
      "serverId": "aea13c4a-0b89-3eca-aee9-e193b1b77aa4",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "ca9ae21b_bab0f4e7",
        "filename": "src/Vulkan/VkDevice.hpp",
        "patchSetId": 6
      },
      "lineNbr": 79,
      "author": {
        "id": 5050
      },
      "writtenOn": "2019-06-12T19:13:19Z",
      "side": 1,
      "message": "For now, I\u0027m using the same logic as Blitter does, which is to mutex lock around the entire query/generate/add section. We should refactor both this code and the Blitter code simultaneously when we get to it, but this is enough to get rid of the memory leak, which is what I\u0027d like to do ASAP.",
      "parentUuid": "585e47c0_4a66a6e6",
      "range": {
        "startLine": 78,
        "startChar": 0,
        "endLine": 79,
        "endChar": 44
      },
      "revId": "998f397d80b59cc8521adf88a73e0cb3d851ccf8",
      "serverId": "aea13c4a-0b89-3eca-aee9-e193b1b77aa4",
      "unresolved": false
    }
  ]
}